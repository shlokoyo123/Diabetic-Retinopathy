{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPhFxuRqu6aiipUR3OUv5ei",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shlokoyo123/Diabetic-Retinopathy/blob/main/DR_Code_Updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUAXZXwb1sEB"
      },
      "outputs": [],
      "source": [
        "# === DR (GPU/Colab) — CELL 1: Train + Save + Batch Inference + Download ===\n",
        "# - Runs the full pipeline\n",
        "# - Saves models/scalers to disk for UI cell\n",
        "# - Runs batch inference on uploaded images\n",
        "# - Generates & downloads artifacts\n",
        "# - No Gradio here\n",
        "\n",
        "# ------------------ CONFIG ------------------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os, io, json, base64, zipfile, hashlib, random, time, math, shutil, sys, re\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image, ImageOps, ImageDraw\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torchvision import models\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.transforms import InterpolationMode\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
        "from tqdm.auto import tqdm\n",
        "from collections import Counter\n",
        "import joblib  # for persisting sklearn objects\n",
        "\n",
        "# --- Paths & knobs ---\n",
        "BASE_UNZIP_DIR      = \"/content/train_subset_unzipped\"\n",
        "IMAGES_DIR_CONFIG   = \"/content/train_subset_unzipped/train_subset\"   # leave blank to auto-detect\n",
        "LABELS_CSV          = \"/content/trainLabels.csv\"\n",
        "ZIP_PATH            = \"/content/drive/MyDrive/train_subset.zip\"       # ok if missing\n",
        "ROOT_OUT            = \"/content/dr_one_cell\"\n",
        "FILES_DIR           = str(Path(ROOT_OUT)/\"public\")\n",
        "MODELS_DIR          = str(Path(ROOT_OUT)/\"models\")\n",
        "\n",
        "# Classification granularity\n",
        "BINARY_MODE         = True   # True: collapse levels >=1 to 1 (referable DR). Helps accuracy on limited data.\n",
        "\n",
        "IMAGE_SIZE          = 224\n",
        "#VAL_FRAC            = 0.40\n",
        "#VAL_FRAC            = 0.20 --> RESNET WAS 74.19\n",
        "#VAL_FRAC            = 0.15 --> RESNET WAS 68...\n",
        "VAL_FRAC            = 0.20\n",
        "SEED                = 1337\n",
        "\n",
        "# Speed/quality\n",
        "#MAX_TRAIN_PER_CLASS = 800\n",
        "MAX_TRAIN_PER_CLASS = 1500\n",
        "BATCH_SIZE          = 64\n",
        "\n",
        "# Autoencoder (improved)\n",
        "AE_EPOCHS             = 0\n",
        "AE_LATENT_DIM         = 384\n",
        "AE_NOISE_STD          = 0.03\n",
        "AE_WEIGHT_DECAY       = 1e-5\n",
        "AE_LR                 = 1e-3\n",
        "AE_SUP_HEAD_EPOCHS    = 0\n",
        "AE_SUP_LR             = 2e-3\n",
        "COLOR_JITTER_STRENGTH = 0.12\n",
        "\n",
        "# Fine-tune (ResNet50 only here; Cell 2 is robust if FT ckpts aren't present)\n",
        "FT_STAGE1_EPOCHS      = 6\n",
        "FT_STAGE2_EPOCHS      = 10\n",
        "FT_BASE_LR            = 2e-4\n",
        "\n",
        "AUTO_DOWNLOAD         = True  # download artifacts at the end of this cell\n",
        "\n",
        "# ------------------ SETUP ------------------\n",
        "torch.manual_seed(SEED); np.random.seed(SEED); random.seed(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "Path(ROOT_OUT).mkdir(parents=True, exist_ok=True)\n",
        "Path(FILES_DIR).mkdir(parents=True, exist_ok=True)\n",
        "Path(MODELS_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ------------------ SAFE UNZIP ------------------\n",
        "import zipfile\n",
        "def safe_unzip(zip_path: str, out_dir: str) -> bool:\n",
        "    if not zip_path or not os.path.exists(zip_path):\n",
        "        print(f\"[info] No zip at {zip_path} (skipping unzip).\"); return False\n",
        "    if os.path.isdir(zip_path):\n",
        "        print(f\"[info] ZIP_PATH is a directory: {zip_path} — skipping unzip.\"); return False\n",
        "    try:\n",
        "        if not zipfile.is_zipfile(zip_path):\n",
        "            print(f\"[warn] Exists but not a valid .zip: {zip_path} — skipping unzip.\"); return False\n",
        "        os.makedirs(out_dir, exist_ok=True)\n",
        "        with zipfile.ZipFile(zip_path, \"r\") as z: z.extractall(out_dir)\n",
        "        print(f\"Extracted {zip_path} to {out_dir}\"); return True\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] Unzip failed: {e} — skipping unzip.\"); return False\n",
        "\n",
        "_ = safe_unzip(ZIP_PATH, BASE_UNZIP_DIR)\n",
        "\n",
        "# ------------------ IMAGE DISCOVERY ------------------\n",
        "def list_images(root):\n",
        "    exts = {\".jpg\",\".jpeg\",\".png\",\".tif\",\".tiff\",\".bmp\"}\n",
        "    root = Path(root)\n",
        "    if not root.exists(): return []\n",
        "    return [str(p) for p in root.rglob(\"*\") if p.suffix.lower() in exts]\n",
        "\n",
        "def dir_has_images(d):\n",
        "    try:\n",
        "        return d and Path(d).exists() and any(p.suffix.lower() in {\".jpg\",\".jpeg\",\".png\",\".tif\",\".tiff\",\".bmp\"} for p in Path(d).rglob(\"*\"))\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def find_image_dir(base_dir: str, min_images: int = 1):\n",
        "    exts = {\".jpg\",\".jpeg\",\".png\",\".tif\",\".tiff\",\".bmp\"}\n",
        "    base = Path(base_dir)\n",
        "    if not base.exists(): return None\n",
        "    if any(p.is_file() and p.suffix.lower() in exts for p in base.iterdir() if p.is_file()):\n",
        "        return str(base)\n",
        "    for sub in base.iterdir():\n",
        "        if sub.is_dir() and any(q.is_file() and q.suffix.lower() in exts for q in sub.iterdir() if q.is_file()):\n",
        "            return str(sub)\n",
        "    imgs = [p for p in base.rglob(\"*\") if p.is_file() and p.suffix.lower() in exts]\n",
        "    if len(imgs) >= min_images: return str(imgs[0].parent)\n",
        "    return None\n",
        "\n",
        "# ------------------ SYNTHETIC FALLBACK ------------------\n",
        "def make_synth_dataset(root_dir, labels_csv, n_per_class=40, size=300):\n",
        "    rnd = random.Random(SEED)\n",
        "    root = Path(root_dir) / \"train_subset\"\n",
        "    root.mkdir(parents=True, exist_ok=True)\n",
        "    rows = []\n",
        "    for cls in range(5):\n",
        "        for i in range(n_per_class):\n",
        "            name = f\"{cls}_{i:03d}\"\n",
        "            img = Image.new(\"RGB\", (size, size), (rnd.randint(10,30), rnd.randint(10,30), rnd.randint(10,30)))\n",
        "            drw = ImageDraw.Draw(img)\n",
        "            r = rnd.randint(16, 28)\n",
        "            cx, cy = rnd.randint(r+10, size-r-10), rnd.randint(r+10, size-r-10)\n",
        "            drw.ellipse([cx-r, cy-r, cx+r, cy+r], outline=(250,240,220), width=3)\n",
        "            for _ in range(cls * 8 + rnd.randint(0,5)):\n",
        "                x, y = rnd.randint(8, size-8), rnd.randint(8, size-8)\n",
        "                rad = rnd.randint(2, 5)\n",
        "                col = (rnd.randint(180,255), rnd.randint(50,120), rnd.randint(50,120))\n",
        "                drw.ellipse([x-rad, y-rad, x+rad, y+rad], fill=col)\n",
        "            p = root / f\"{name}.jpg\"\n",
        "            img.save(p, quality=92)\n",
        "            rows.append({\"image\": name, \"level\": cls})\n",
        "    pd.DataFrame(rows).to_csv(labels_csv, index=False)\n",
        "    print(f\"[synth] Wrote {len(rows)} images to {root} and labels to {labels_csv}\")\n",
        "    return str(root)\n",
        "\n",
        "# Resolve images dir or synthesize\n",
        "IMAGES_DIR = IMAGES_DIR_CONFIG\n",
        "if not dir_has_images(IMAGES_DIR):\n",
        "    guess = find_image_dir(BASE_UNZIP_DIR, min_images=1)\n",
        "    if guess:\n",
        "        print(f\"[auto-detect] Using images from: {guess}\")\n",
        "        IMAGES_DIR = guess\n",
        "if not dir_has_images(IMAGES_DIR):\n",
        "    IMAGES_DIR = make_synth_dataset(BASE_UNZIP_DIR, LABELS_CSV, n_per_class=40, size=300)\n",
        "\n",
        "print(\"[debug] IMAGES_DIR =\", IMAGES_DIR)\n",
        "\n",
        "# ------------------ LOAD LABELS & MATCH ------------------\n",
        "labels = pd.read_csv(LABELS_CSV)\n",
        "assert {\"image\",\"level\"}.issubset(labels.columns), \"LABELS_CSV must have columns: image, level\"\n",
        "labels[\"image\"] = labels[\"image\"].astype(str)\n",
        "labels[\"level\"] = labels[\"level\"].astype(int)\n",
        "if BINARY_MODE:\n",
        "    labels[\"level\"] = (labels[\"level\"] >= 1).astype(int)\n",
        "\n",
        "label_map = dict(zip(labels[\"image\"], labels[\"level\"]))\n",
        "def stem_only(p): return Path(p).stem\n",
        "\n",
        "rows = []\n",
        "all_imgs = list_images(IMAGES_DIR)\n",
        "for p in all_imgs:\n",
        "    stem = stem_only(p)\n",
        "    lvl = label_map.get(stem)\n",
        "    if lvl is not None:\n",
        "        rows.append({\"filename\": p, \"level\": int(lvl)})\n",
        "df_all = pd.DataFrame(rows)\n",
        "if df_all.empty:\n",
        "    print(\"[fatal] No matched images. Check filename stems vs CSV 'image' column.\")\n",
        "    raise SystemExit(1)\n",
        "\n",
        "# ------------------ PREPROCESS ------------------\n",
        "def crop_black(im, thresh=8):\n",
        "    g = im.convert(\"L\")\n",
        "    bw = g.point(lambda x: 255 if x>thresh else 0, mode=\"1\")\n",
        "    bbox = bw.getbbox()\n",
        "    return im.crop(bbox) if bbox else im\n",
        "\n",
        "def preprocess_all(df_all, proc_dir, size):\n",
        "    Path(proc_dir).mkdir(parents=True, exist_ok=True)\n",
        "    out_paths = []\n",
        "    print(f\"Preprocessing {len(df_all)} images to {size}x{size}...\")\n",
        "    for p in tqdm(df_all[\"filename\"]):\n",
        "        outp = str(Path(proc_dir)/Path(p).name)\n",
        "        if not Path(outp).exists():\n",
        "            try:\n",
        "                with Image.open(p) as im:\n",
        "                    im = ImageOps.exif_transpose(im).convert(\"RGB\")\n",
        "                    im = crop_black(im)\n",
        "                    im = ImageOps.autocontrast(im, cutoff=1)          # less aggressive than default\n",
        "                    im = ImageOps.equalize(im)                        # boosts vessel contrast\n",
        "                    # center-crop square then resize (fundus is circular; corners are junk)\n",
        "                    w,h = im.size\n",
        "                    s = min(w,h)\n",
        "                    im = im.crop(((w-s)//2, (h-s)//2, (w+s)//2, (h+s)//2))\n",
        "                    im = im.resize((size,size))\n",
        "                    im.save(outp, quality=95)\n",
        "            except Exception as e:\n",
        "                print(\"[pre] skip\", p, e)\n",
        "                continue\n",
        "        out_paths.append(outp)\n",
        "    df = df_all.copy()\n",
        "    df[\"proc_path\"] = out_paths\n",
        "    return df\n",
        "\n",
        "PROC_DIR = Path(ROOT_OUT)/f\"processed_{IMAGE_SIZE}\"\n",
        "df_all = preprocess_all(df_all, PROC_DIR, IMAGE_SIZE)\n",
        "df_all.to_csv(Path(ROOT_OUT)/\"_matched_files.csv\", index=False)\n",
        "\n",
        "# ------------------ SPLIT ------------------\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=VAL_FRAC, random_state=SEED)\n",
        "idx_tr, idx_va = next(sss.split(df_all[\"proc_path\"], df_all[\"level\"]))\n",
        "DF_TR = df_all.iloc[idx_tr].reset_index(drop=True)\n",
        "DF_VA = df_all.iloc[idx_va].reset_index(drop=True)\n",
        "print(f\"[split] train={len(DF_TR)} val={len(DF_VA)}\")\n",
        "\n",
        "# ------------------ CACHED TENSORS ------------------\n",
        "MEAN = torch.tensor([0.485,0.456,0.406]).view(3,1,1)\n",
        "STD  = torch.tensor([0.229,0.224,0.225]).view(3,1,1)\n",
        "CACHE_DIR = Path(ROOT_OUT)/\"_tensor_cache\"\n",
        "\n",
        "def pil_to_tensor_resized(path, size):\n",
        "    with Image.open(path) as im:\n",
        "        im = ImageOps.exif_transpose(im).convert(\"RGB\").resize((size,size))\n",
        "        arr = np.asarray(im, dtype=np.float32)/255.0\n",
        "        arr = np.transpose(arr, (2,0,1))\n",
        "        t = torch.from_numpy(arr)\n",
        "        t = (t - MEAN) / STD\n",
        "        return t\n",
        "\n",
        "def cache_key(path, size):\n",
        "    b = Path(path).name\n",
        "    mtime = int(Path(path).stat().st_mtime)\n",
        "    return hashlib.sha1(f\"{b}|{size}|{mtime}\".encode()).hexdigest()+\".pt\"\n",
        "\n",
        "def cached_tensor_for(path, size, cache_dir):\n",
        "    cache_dir = Path(cache_dir); cache_dir.mkdir(parents=True, exist_ok=True)\n",
        "    fp = cache_dir/cache_key(path, size)\n",
        "    if fp.exists():\n",
        "        try: return torch.load(fp, map_location=\"cpu\")\n",
        "        except Exception:\n",
        "            try: fp.unlink()\n",
        "            except: pass\n",
        "    t = pil_to_tensor_resized(path, size)\n",
        "    torch.save(t, fp)\n",
        "    return t\n",
        "\n",
        "class CachedTensorDS(Dataset):\n",
        "    def __init__(self, df, size, cache_dir):\n",
        "        self.df = df.reset_index(drop=True); self.size=size; self.cache_dir=cache_dir\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, i):\n",
        "        r = self.df.iloc[i]\n",
        "        x = cached_tensor_for(r.proc_path, self.size, self.cache_dir)\n",
        "        y = int(r.level)\n",
        "        return x, y\n",
        "\n",
        "def make_loader(df, bs, shuffle=False, sampler=None):\n",
        "    return DataLoader(CachedTensorDS(df, IMAGE_SIZE, CACHE_DIR),\n",
        "                      batch_size=bs, shuffle=(sampler is None and shuffle),\n",
        "                      sampler=sampler, num_workers=0, pin_memory=False)\n",
        "\n",
        "# ------------------ CAP + SAMPLER ------------------\n",
        "def stratified_cap(df, label_col=\"level\", cap=MAX_TRAIN_PER_CLASS, seed=SEED):\n",
        "    if cap is None or cap<=0: return df.copy().reset_index(drop=True)\n",
        "    parts=[]\n",
        "    for lvl, g in df.groupby(label_col):\n",
        "        parts.append(g.sample(n=min(len(g), cap), random_state=seed))\n",
        "    return pd.concat(parts, ignore_index=True)\n",
        "\n",
        "DF_TR_CAP = stratified_cap(DF_TR, \"level\", MAX_TRAIN_PER_CLASS, SEED)\n",
        "print(f\"[speed] Using {len(DF_TR_CAP)} capped train / {len(DF_VA)} val\")\n",
        "\n",
        "counts = Counter(DF_TR_CAP[\"level\"].tolist())\n",
        "num_classes = 2 if BINARY_MODE else 5\n",
        "class_counts = np.array([counts.get(i, 1) for i in range(num_classes)], dtype=np.float32)\n",
        "class_weights_for_sampler = (class_counts.sum() / (class_counts+1e-6))\n",
        "sample_weights = DF_TR_CAP[\"level\"].map({i:class_weights_for_sampler[i] for i in range(num_classes)}).values\n",
        "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "TR_LOADER = make_loader(DF_TR_CAP, BATCH_SIZE, shuffle=False, sampler=sampler)\n",
        "VA_LOADER = make_loader(DF_VA,       BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# ------------------ IMPROVED AUTOENCODER ------------------\n",
        "class ImprovedAE(nn.Module):\n",
        "    def __init__(self, latent_dim=AE_LATENT_DIM):\n",
        "        super().__init__()\n",
        "        ch = [3, 64, 128, 256, 512]\n",
        "        enc = []\n",
        "        for i in range(len(ch)-1):\n",
        "            enc += [\n",
        "                nn.Conv2d(ch[i], ch[i+1], 3, stride=2, padding=1, bias=False),\n",
        "                nn.BatchNorm2d(ch[i+1]),\n",
        "                nn.GELU(),\n",
        "            ]\n",
        "        self.enc_conv = nn.Sequential(*enc)  # 224 -> 112 -> 56 -> 28 -> 14\n",
        "        self.enc_gap  = nn.AdaptiveAvgPool2d(1)\n",
        "        self.enc_fc   = nn.Linear(512, latent_dim)\n",
        "\n",
        "        self.dec_fc   = nn.Linear(latent_dim, 512*14*14)\n",
        "        self.dec_deconv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(512, 256, 3, stride=2, padding=1, output_padding=1), nn.BatchNorm2d(256), nn.GELU(),\n",
        "            nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1), nn.BatchNorm2d(128), nn.GELU(),\n",
        "            nn.ConvTranspose2d(128,  64, 3, stride=2, padding=1, output_padding=1), nn.BatchNorm2d(64),  nn.GELU(),\n",
        "            nn.ConvTranspose2d(64,     3, 3, stride=2, padding=1, output_padding=1),\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.enc_conv(x)\n",
        "        h = self.enc_gap(h).flatten(1)\n",
        "        z = self.enc_fc(h)\n",
        "        return z\n",
        "\n",
        "    def decode(self, z):\n",
        "        h = self.dec_fc(z).view(-1, 512, 14, 14)\n",
        "        x = self.dec_deconv(h)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encode(x)\n",
        "        xh = self.decode(z)\n",
        "        return xh\n",
        "\n",
        "def _denorm01(x):\n",
        "    mean = MEAN.to(x.device); std = STD.to(x.device)\n",
        "    return (x * std) + mean\n",
        "def _renorm(x01):\n",
        "    mean = MEAN.to(x01.device); std = STD.to(x01.device)\n",
        "    return (x01 - mean) / std\n",
        "\n",
        "def ae_augment_tensor(x, cj=COLOR_JITTER_STRENGTH):\n",
        "    B, C, H, W = x.shape\n",
        "    out = x\n",
        "    if torch.rand(1, device=x.device) < 0.5: out = torch.flip(out, dims=[3])\n",
        "    if torch.rand(1, device=x.device) < 0.2: out = torch.flip(out, dims=[2])\n",
        "    angle = float(torch.empty(1, device=x.device).uniform_(-10.0, 10.0))\n",
        "    scale = float(torch.empty(1, device=x.device).uniform_(0.95, 1.05))\n",
        "    shear = float(torch.empty(1, device=x.device).uniform_(-4.0, 4.0))\n",
        "    affined = []\n",
        "    for i in range(B):\n",
        "        img = out[i]\n",
        "        img = TF.affine(img, angle=angle, translate=(0, 0), scale=scale, shear=[shear, 0.0],\n",
        "                        interpolation=InterpolationMode.BILINEAR)\n",
        "        affined.append(img)\n",
        "    out = torch.stack(affined, dim=0)\n",
        "    if cj and cj > 0:\n",
        "        x01 = _denorm01(out).clamp(0,1)\n",
        "        b = 1.0 + float(torch.empty(1, device=x.device).uniform_(-cj, cj))\n",
        "        c = 1.0 + float(torch.empty(1, device=x.device).uniform_(-cj, cj))\n",
        "        s = 1.0 + float(torch.empty(1, device=x.device).uniform_(-cj, cj))\n",
        "        x01 = TF.adjust_brightness(x01, b)\n",
        "        x01 = TF.adjust_contrast(x01,  c)\n",
        "        x01 = TF.adjust_saturation(x01, s)\n",
        "        out = _renorm(x01.clamp(0,1))\n",
        "    return out\n",
        "\n",
        "print(f\"\\n[AE] Training Improved Autoencoder for {AE_EPOCHS} epochs...\")\n",
        "if Path(CACHE_DIR).exists():\n",
        "    try:\n",
        "        shutil.rmtree(CACHE_DIR)\n",
        "        print(f\"[info] Cleared tensor cache: {CACHE_DIR}\")\n",
        "    except Exception: pass\n",
        "\n",
        "TR_LOADER_AE = make_loader(DF_TR_CAP, BATCH_SIZE, shuffle=True)\n",
        "ae_model = ImprovedAE(latent_dim=AE_LATENT_DIM).to(device)\n",
        "recon_crit = nn.MSELoss()\n",
        "optimizer = torch.optim.AdamW(ae_model.parameters(), lr=AE_LR, weight_decay=AE_WEIGHT_DECAY)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max(1, AE_EPOCHS))\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(device.type==\"cuda\"))\n",
        "\n",
        "ae_model.train()\n",
        "for ep in range(1, AE_EPOCHS+1):\n",
        "    run = 0.0\n",
        "    pbar = tqdm(TR_LOADER_AE, desc=f\"AE {ep}/{AE_EPOCHS}\")\n",
        "    for xb, _ in pbar:\n",
        "        xb = xb.to(device)\n",
        "        x_in = ae_augment_tensor(xb)\n",
        "        if AE_NOISE_STD > 0:\n",
        "            x_in = (x_in + torch.randn_like(x_in)*AE_NOISE_STD).clamp(-5, 5)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\")):\n",
        "            xh = ae_model(x_in); loss = recon_crit(xh, xb)\n",
        "        scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()\n",
        "        run += loss.item(); pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
        "    scheduler.step()\n",
        "    print(f\"[AE] ep {ep}: recon={run/len(TR_LOADER_AE):.4f}\")\n",
        "ae_model.eval()\n",
        "\n",
        "# Optional supervised head (frozen encoder)\n",
        "class AESupHead(nn.Module):\n",
        "    def __init__(self, encoder: ImprovedAE, num_classes=2 if BINARY_MODE else 5, in_dim=AE_LATENT_DIM):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        for p in self.encoder.parameters(): p.requires_grad = False\n",
        "        self.head = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x):\n",
        "        with torch.no_grad():\n",
        "            z = self.encoder.encode(x)\n",
        "        return self.head(z)\n",
        "\n",
        "if AE_SUP_HEAD_EPOCHS > 0:\n",
        "    print(f\"[AE] Supervised head fine-tune ({AE_SUP_HEAD_EPOCHS} epochs; frozen encoder)\")\n",
        "    sup_model = AESupHead(ae_model).to(device)\n",
        "    opt_sup = torch.optim.AdamW(sup_model.head.parameters(), lr=AE_SUP_LR)\n",
        "    ce = nn.CrossEntropyLoss()\n",
        "    sup_model.train()\n",
        "    for ep in range(1, AE_SUP_HEAD_EPOCHS+1):\n",
        "        run_loss, seen, cor = 0.0, 0, 0\n",
        "        for xb, yb in TR_LOADER:\n",
        "            xb, yb = xb.to(device), torch.as_tensor(yb, device=device)\n",
        "            logits = sup_model(xb); loss = ce(logits, yb)\n",
        "            opt_sup.zero_grad(set_to_none=True); loss.backward(); opt_sup.step()\n",
        "            run_loss += loss.item()*yb.size(0); seen += yb.size(0)\n",
        "            cor += (logits.argmax(1)==yb).sum().item()\n",
        "        print(f\"[AE-sup] ep {ep}: train_acc={cor/max(1,seen):.3f} loss={(run_loss/max(1,seen)):.4f}\")\n",
        "    sup_model.eval()\n",
        "\n",
        "# ------------------ BACKBONES & LINEAR HEADS ------------------\n",
        "def build_resnet50_feats():\n",
        "    try:\n",
        "        m = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
        "    except Exception as e:\n",
        "        print(\"[warn] ResNet50 weights download failed, using random init:\", e)\n",
        "        m = models.resnet50(weights=None)\n",
        "    m.fc = nn.Identity(); m.eval(); return m.to(device)\n",
        "\n",
        "def build_vit_b16_feats():\n",
        "    try:\n",
        "        m = models.vit_b_16(weights=models.ViT_B_16_Weights.IMAGENET1K_V1)\n",
        "    except Exception as e:\n",
        "        print(\"[warn] ViT-B/16 weights download failed, using random init:\", e)\n",
        "        m = models.vit_b_16(weights=None)\n",
        "    m.heads = nn.Identity(); m.eval(); return m.to(device)\n",
        "\n",
        "def build_ae_feats(trained_ae: ImprovedAE):\n",
        "    class EncOnly(nn.Module):\n",
        "        def __init__(self, ae: ImprovedAE):\n",
        "            super().__init__()\n",
        "            self.enc_conv = ae.enc_conv\n",
        "            self.enc_gap  = ae.enc_gap\n",
        "            self.enc_fc   = ae.enc_fc\n",
        "        def forward(self, x):\n",
        "            h = self.enc_conv(x); h = self.enc_gap(h).flatten(1); z = self.enc_fc(h)\n",
        "            return z\n",
        "    return EncOnly(trained_ae).eval().to(device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def extract_features(backbone, loader):\n",
        "    feats, ys = [], []\n",
        "    for xb, yb in tqdm(loader, desc=\"Extracting\"):\n",
        "        xb = xb.to(device)\n",
        "        out = backbone(xb)\n",
        "        if out.dim()==4: out = out.mean((2,3))\n",
        "        feats.append(out.cpu().numpy()); ys.append(np.asarray(yb))\n",
        "    return np.concatenate(feats), np.concatenate(ys)\n",
        "\n",
        "def fit_eval_linear(Xtr, ytr, Xva, yva, label):\n",
        "    scaler = StandardScaler()\n",
        "    Xtr_s = scaler.fit_transform(Xtr); Xva_s = scaler.transform(Xva)\n",
        "    clf = LogisticRegression(max_iter=600, class_weight=\"balanced\").fit(Xtr_s, ytr)\n",
        "    pred = clf.predict(Xva_s)\n",
        "    acc = accuracy_score(yva, pred)\n",
        "    qwk = cohen_kappa_score(yva, pred, weights=\"quadratic\")\n",
        "    print(f\"[{label}] ACC={acc:.4f} QWK={qwk:.4f}\")\n",
        "    return acc, qwk, scaler, clf\n",
        "\n",
        "TR_LOADER_NS = make_loader(DF_TR_CAP, BATCH_SIZE, shuffle=False)\n",
        "VA_LOADER_NS = make_loader(DF_VA,       BATCH_SIZE, shuffle=False)\n",
        "results = []\n",
        "\n",
        "print(\"\\n[FAST] ResNet50 features…\")\n",
        "resnet = build_resnet50_feats()\n",
        "Xtr_r, ytr = extract_features(resnet, TR_LOADER_NS)\n",
        "Xva_r, yva = extract_features(resnet, VA_LOADER_NS)\n",
        "acc_r, qwk_r, sc_r, clf_r = fit_eval_linear(Xtr_r, ytr, Xva_r, yva, \"resnet50_linear\")\n",
        "results.append({\"model\":\"resnet50_linear\",\"val_acc\":acc_r,\"val_qwk\":qwk_r})\n",
        "\n",
        "print(\"\\n[FAST] ViT-B/16 features…\")\n",
        "vit = build_vit_b16_feats()\n",
        "Xtr_v, _ = extract_features(vit, TR_LOADER_NS)\n",
        "Xva_v, _ = extract_features(vit, VA_LOADER_NS)\n",
        "acc_v, qwk_v, sc_v, clf_v = fit_eval_linear(Xtr_v, ytr, Xva_v, yva, \"vit_b16_linear\")\n",
        "results.append({\"model\":\"vit_b16_linear\",\"val_acc\":acc_v,\"val_qwk\":qwk_v})\n",
        "\n",
        "print(\"\\n[FAST] Improved AE encoder features…\")\n",
        "ae_backbone = build_ae_feats(ae_model)\n",
        "Xtr_ae, _ = extract_features(ae_backbone, TR_LOADER_NS)\n",
        "Xva_ae, _ = extract_features(ae_backbone, VA_LOADER_NS)\n",
        "acc_ae, qwk_ae, sc_ae, clf_ae = fit_eval_linear(Xtr_ae, ytr, Xva_ae, yva, \"autoencoder_linear\")\n",
        "results.append({\"model\":\"autoencoder_linear\",\"val_acc\":acc_ae,\"val_qwk\":qwk_ae})\n",
        "\n",
        "# ------------------ SUMMARY + PLOTS ------------------\n",
        "summary_df = pd.DataFrame(results).sort_values([\"val_qwk\",\"val_acc\"], ascending=False).reset_index(drop=True)\n",
        "summary_csv = Path(ROOT_OUT)/\"summary_models.csv\"\n",
        "summary_df.to_csv(summary_csv, index=False)\n",
        "\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.bar(summary_df[\"model\"], summary_df[\"val_acc\"]); plt.ylim(0,1)\n",
        "plt.ylabel(\"Val ACC\"); plt.title(\"Accuracy by Model\"); plt.grid(axis=\"y\", alpha=0.3); plt.xticks(rotation=12)\n",
        "plt.tight_layout(); plt.savefig(Path(ROOT_OUT)/\"acc_by_model.png\", dpi=140); plt.close()\n",
        "\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.bar(summary_df[\"model\"], summary_df[\"val_qwk\"]); plt.ylim(0,1)\n",
        "plt.ylabel(\"Val QWK\"); plt.title(\"QWK by Model\"); plt.grid(axis=\"y\", alpha=0.3); plt.xticks(rotation=12)\n",
        "plt.tight_layout(); plt.savefig(Path(ROOT_OUT)/\"qwk_by_model.png\", dpi=140); plt.close()\n",
        "\n",
        "print(\"\\n=== SUMMARY (higher is better) ===\")\n",
        "print(summary_df)\n",
        "\n",
        "# ------------------ Fine-tune ResNet50 (end-to-end) ------------------\n",
        "def build_train_pil_dataset(df):\n",
        "    class DRTrainPIL(Dataset):\n",
        "        def __init__(self, df, size):\n",
        "            self.df = df.reset_index(drop=True); self.size = size\n",
        "            self.tf = T.Compose([\n",
        "                T.Resize(int(size*1.10), interpolation=InterpolationMode.BILINEAR),\n",
        "                T.RandomResizedCrop(size, scale=(0.85, 1.0), ratio=(0.9, 1.1)),\n",
        "                T.RandomHorizontalFlip(p=0.5),\n",
        "                T.RandomVerticalFlip(p=0.2),\n",
        "                T.RandomRotation(degrees=10, interpolation=InterpolationMode.BILINEAR, fill=0),\n",
        "                T.ColorJitter(brightness=0.10, contrast=0.25, saturation=0.10, hue=0.00),\n",
        "                T.RandomPerspective(distortion_scale=0.15, p=0.25, interpolation=InterpolationMode.BILINEAR),\n",
        "                T.ToTensor(),\n",
        "                T.Normalize(mean=MEAN.view(-1).tolist(), std=STD.view(-1).tolist()),\n",
        "            ])\n",
        "        def __len__(self): return len(self.df)\n",
        "        def __getitem__(self, i):\n",
        "            r = self.df.iloc[i]\n",
        "            with Image.open(r.proc_path) as im:\n",
        "                im = ImageOps.exif_transpose(im).convert(\"RGB\")\n",
        "                x = self.tf(im)\n",
        "            return x, int(r.level)\n",
        "    return DRTrainPIL(df, IMAGE_SIZE)\n",
        "\n",
        "def make_train_aug_loader(df, bs, sampler=None):\n",
        "    ds = build_train_pil_dataset(df)\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=bs,\n",
        "        shuffle=(sampler is None),\n",
        "        sampler=sampler,\n",
        "        num_workers=2,\n",
        "        pin_memory=(device.type==\"cuda\"),\n",
        "    )\n",
        "\n",
        "TR_LOADER_AUG = make_train_aug_loader(DF_TR_CAP, bs=16, sampler=None)\n",
        "\n",
        "freq = torch.tensor([counts.get(c, 1) for c in range(num_classes)], dtype=torch.float32)\n",
        "class_weights = (1.0 / freq).to(device)\n",
        "class_weights = class_weights * (num_classes / class_weights.sum())\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2.0):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "    def forward(self, logits, target, weight=None):\n",
        "        logp = F.log_softmax(logits, dim=1)\n",
        "        p = torch.exp(logp)\n",
        "        pt = p.gather(1, target.unsqueeze(1)).squeeze(1)\n",
        "        logpt = logp.gather(1, target.unsqueeze(1)).squeeze(1)\n",
        "        focal = (1 - pt).clamp(0,1) ** self.gamma\n",
        "        loss = -focal * logpt\n",
        "        if weight is not None:\n",
        "            loss = loss * weight[target]\n",
        "        return loss.mean()\n",
        "\n",
        "def build_resnet50_for_ft(num_classes=num_classes):\n",
        "    try:\n",
        "        m = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
        "    except Exception as e:\n",
        "        print(\"[warn] ResNet50 weights download failed for FT, using random init:\", e)\n",
        "        m = models.resnet50(weights=None)\n",
        "    in_features = m.fc.in_features\n",
        "    m.fc = nn.Linear(in_features, num_classes)\n",
        "    return m.to(device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_with_tta(model, loader, tta=2):\n",
        "    model.eval(); all_y, all_p = [], []\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(device); yb = yb.to(device)\n",
        "        logits_sum = 0\n",
        "        for t in range(tta):\n",
        "            x_in = xb\n",
        "            if t == 1: x_in = torch.flip(x_in, dims=[3])\n",
        "            logits_sum = logits_sum + model(x_in)\n",
        "        logits = logits_sum / float(tta)\n",
        "        all_p.append(logits.argmax(1).cpu()); all_y.append(yb.cpu())\n",
        "    y_true = torch.cat(all_y).numpy(); y_pred = torch.cat(all_p).numpy()\n",
        "    return accuracy_score(y_true, y_pred), cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n",
        "\n",
        "def finetune_resnet50(epochs_stage1=FT_STAGE1_EPOCHS, epochs_stage2=FT_STAGE2_EPOCHS, base_lr=FT_BASE_LR, min_lr=1e-6):\n",
        "    model = build_resnet50_for_ft(num_classes=num_classes)\n",
        "\n",
        "    for p in model.parameters(): p.requires_grad = False\n",
        "    for p in model.layer3.parameters(): p.requires_grad = True\n",
        "    for p in model.layer4.parameters(): p.requires_grad = True\n",
        "    for p in model.fc.parameters():     p.requires_grad = True\n",
        "\n",
        "    crit = FocalLoss(gamma=2.0)\n",
        "    opt  = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=base_lr, weight_decay=1e-4)\n",
        "    sched= torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=max(1,epochs_stage1), eta_min=max(min_lr, base_lr*0.1))\n",
        "\n",
        "    best = {\"qwk\": -1, \"state\": None}\n",
        "    for ep in range(1, epochs_stage1+1):\n",
        "        model.train()\n",
        "        for xb, yb in TR_LOADER_AUG:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            logits = model(xb); loss = crit(logits, yb, weight=class_weights)\n",
        "            loss.backward(); opt.step()\n",
        "        sched.step()\n",
        "        acc, qwk = evaluate_with_tta(model, VA_LOADER, tta=4)\n",
        "        print(f\"[FT-stage1] ep {ep}: acc={acc:.4f} qwk={qwk:.4f}\")\n",
        "        if qwk > best[\"qwk\"]:\n",
        "            best = {\"qwk\": qwk, \"state\": {k: v.clone() for k, v in model.state_dict().items()}}\n",
        "\n",
        "    for p in model.parameters(): p.requires_grad = True\n",
        "    opt  = torch.optim.AdamW(model.parameters(), lr=base_lr*0.5, weight_decay=1e-4)\n",
        "    sched= torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=max(1,epochs_stage2), eta_min=min_lr)\n",
        "    patience = 4; no_improve = 0\n",
        "\n",
        "    for ep in range(1, epochs_stage2+1):\n",
        "        model.train()\n",
        "        for xb, yb in TR_LOADER_AUG:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            logits = model(xb); loss = crit(logits, yb, weight=class_weights)\n",
        "            loss.backward(); opt.step()\n",
        "        sched.step()\n",
        "        acc, qwk = evaluate_with_tta(model, VA_LOADER, tta=2)\n",
        "        print(f\"[FT-stage2] ep {ep}: acc={acc:.4f} qwk={qwk:.4f}\")\n",
        "        if qwk > best[\"qwk\"]:\n",
        "            best = {\"qwk\": qwk, \"state\": {k: v.clone() for k, v in model.state_dict().items()}}; no_improve = 0\n",
        "        else:\n",
        "            no_improve += 1\n",
        "            if no_improve >= patience:\n",
        "                print(\"[early-stop] no QWK improvement\"); break\n",
        "\n",
        "    if best[\"state\"] is not None: model.load_state_dict(best[\"state\"])\n",
        "    final_acc, final_qwk = evaluate_with_tta(model, VA_LOADER, tta=2)\n",
        "    print(f\"[FT-final] acc={final_acc:.4f} qwk={final_qwk:.4f}\")\n",
        "    return model, final_acc, final_qwk\n",
        "\n",
        "print(\"\\n[FT] Fine-tuning ResNet50 end-to-end …\")\n",
        "ft_model, ft_acc, ft_qwk = finetune_resnet50()\n",
        "summary_df = pd.concat([summary_df, pd.DataFrame([{\"model\":\"resnet50_finetune\",\"val_acc\":ft_acc,\"val_qwk\":ft_qwk}])], ignore_index=True)\n",
        "summary_df = summary_df.sort_values([\"val_qwk\",\"val_acc\"], ascending=False).reset_index(drop=True)\n",
        "summary_df.to_csv(Path(ROOT_OUT)/\"summary_models.csv\", index=False)\n",
        "print(\"\\n=== UPDATED SUMMARY (with fine-tune) ===\")\n",
        "print(summary_df)\n",
        "\n",
        "# ==========================================================================================\n",
        "# ===================== EXTRA RESULTS: CM + ROC + QUALITY + CONF + CASES + DIAGRAM + REDUCTION\n",
        "# ==========================================================================================\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
        "import itertools\n",
        "\n",
        "EXTRA_DIR = Path(ROOT_OUT) / \"extra_results\"\n",
        "EXTRA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def save_confusion_matrix(y_true, y_pred, labels, title, out_png):\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "    plt.figure(figsize=(5.2,4.6))\n",
        "    plt.imshow(cm, interpolation=\"nearest\")\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(labels))\n",
        "    plt.xticks(tick_marks, labels, rotation=20)\n",
        "    plt.yticks(tick_marks, labels)\n",
        "\n",
        "    thresh = cm.max() * 0.6 if cm.max() > 0 else 0\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], \"d\"),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel(\"True label\")\n",
        "    plt.xlabel(\"Predicted label\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_png, dpi=160)\n",
        "    plt.close()\n",
        "    return cm\n",
        "\n",
        "def save_roc_curve(y_true_bin, y_score, title, out_png):\n",
        "    fpr, tpr, _ = roc_curve(y_true_bin, y_score)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(5.6,4.6))\n",
        "    plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.3f}\")\n",
        "    plt.plot([0,1], [0,1], linestyle=\"--\", label=\"Chance\")\n",
        "    plt.xlim(0,1); plt.ylim(0,1.02)\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(title)\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(alpha=0.25)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_png, dpi=160)\n",
        "    plt.close()\n",
        "    return roc_auc\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_ft_probs(model, loader):\n",
        "    model.eval()\n",
        "    probs = []\n",
        "    preds = []\n",
        "    ys = []\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(device)\n",
        "        logits = model(xb)\n",
        "        p = torch.softmax(logits, dim=1)\n",
        "        probs.append(p.detach().cpu().numpy())\n",
        "        preds.append(torch.argmax(p, dim=1).cpu().numpy())\n",
        "        ys.append(np.asarray(yb))\n",
        "    return np.concatenate(probs), np.concatenate(preds), np.concatenate(ys)\n",
        "\n",
        "def annotate_case(img_path, lines, out_path):\n",
        "    with Image.open(img_path) as im:\n",
        "        im = ImageOps.exif_transpose(im).convert(\"RGB\")\n",
        "        im = im.resize((IMAGE_SIZE, IMAGE_SIZE))\n",
        "        draw = ImageDraw.Draw(im)\n",
        "        y = 6\n",
        "        for ln in lines:\n",
        "            draw.rectangle([2, y-1, IMAGE_SIZE-2, y+16], fill=(0,0,0))\n",
        "            draw.text((6, y), ln, fill=(255,255,255))\n",
        "            y += 18\n",
        "        im.save(out_path, quality=95)\n",
        "\n",
        "# (1) CONFUSION MATRICES (VAL)\n",
        "labels_list = [0,1] if BINARY_MODE else list(range(5))\n",
        "\n",
        "def preds_from_linear(Xva, scaler, clf):\n",
        "    Xs = scaler.transform(Xva)\n",
        "    pred = clf.predict(Xs)\n",
        "    proba = clf.predict_proba(Xs) if hasattr(clf,\"predict_proba\") else None\n",
        "    return pred, proba\n",
        "\n",
        "pred_r, proba_r = preds_from_linear(Xva_r, sc_r, clf_r)\n",
        "pred_v, proba_v = preds_from_linear(Xva_v, sc_v, clf_v)\n",
        "pred_ae, proba_ae = preds_from_linear(Xva_ae, sc_ae, clf_ae)\n",
        "\n",
        "_ = save_confusion_matrix(yva, pred_r, labels_list,\n",
        "                          \"Confusion Matrix: ResNet50 Linear (Val)\",\n",
        "                          EXTRA_DIR/\"cm_resnet50_linear.png\")\n",
        "_ = save_confusion_matrix(yva, pred_v, labels_list,\n",
        "                          \"Confusion Matrix: ViT-B/16 Linear (Val)\",\n",
        "                          EXTRA_DIR/\"cm_vit_b16_linear.png\")\n",
        "_ = save_confusion_matrix(yva, pred_ae, labels_list,\n",
        "                          \"Confusion Matrix: Autoencoder Linear (Val)\",\n",
        "                          EXTRA_DIR/\"cm_autoencoder_linear.png\")\n",
        "\n",
        "ft_probs, ft_pred, ft_y = predict_ft_probs(ft_model, VA_LOADER)\n",
        "_ = save_confusion_matrix(ft_y, ft_pred, labels_list,\n",
        "                          \"Confusion Matrix: ResNet50 Fine-tuned (Val)\",\n",
        "                          EXTRA_DIR/\"cm_resnet50_finetune.png\")\n",
        "\n",
        "print(\"[extra] Confusion matrices saved to:\", EXTRA_DIR)\n",
        "\n",
        "# (2) ROC CURVES (binary)\n",
        "if BINARY_MODE:\n",
        "    y_true_bin = (yva == 1).astype(int)\n",
        "    auc_r = save_roc_curve(y_true_bin, proba_r[:,1], \"ROC: ResNet50 Linear (Val)\", EXTRA_DIR/\"roc_resnet50_linear.png\")\n",
        "    auc_v = save_roc_curve(y_true_bin, proba_v[:,1], \"ROC: ViT-B/16 Linear (Val)\", EXTRA_DIR/\"roc_vit_b16_linear.png\")\n",
        "    auc_ae= save_roc_curve(y_true_bin, proba_ae[:,1], \"ROC: Autoencoder Linear (Val)\", EXTRA_DIR/\"roc_autoencoder_linear.png\")\n",
        "    auc_ft= save_roc_curve((ft_y==1).astype(int), ft_probs[:,1], \"ROC: ResNet50 Fine-tuned (Val)\", EXTRA_DIR/\"roc_resnet50_finetune.png\")\n",
        "    print(f\"[extra] AUCs: resnet_lin={auc_r:.3f} vit_lin={auc_v:.3f} ae_lin={auc_ae:.3f} resnet_ft={auc_ft:.3f}\")\n",
        "else:\n",
        "    print(\"[extra] ROC curves skipped (BINARY_MODE=False).\")\n",
        "\n",
        "# (3) CONFIDENCE COMPARISON (VAL)\n",
        "def max_conf_from_proba(proba):\n",
        "    return proba.max(axis=1)\n",
        "\n",
        "val_conf = pd.DataFrame({\n",
        "    \"model\": [\"resnet50_linear\",\"vit_b16_linear\",\"autoencoder_linear\",\"resnet50_finetune\"],\n",
        "    \"mean_max_conf\": [\n",
        "        float(max_conf_from_proba(proba_r).mean()),\n",
        "        float(max_conf_from_proba(proba_v).mean()),\n",
        "        float(max_conf_from_proba(proba_ae).mean()),\n",
        "        float(np.max(ft_probs, axis=1).mean()),\n",
        "    ]\n",
        "})\n",
        "val_conf.to_csv(EXTRA_DIR/\"val_confidence_comparison.csv\", index=False)\n",
        "\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.bar(val_conf[\"model\"], val_conf[\"mean_max_conf\"])\n",
        "plt.ylim(0,1)\n",
        "plt.ylabel(\"Mean max probability\")\n",
        "plt.title(\"Validation: Mean Confidence (Max Prob) by Model\")\n",
        "plt.grid(axis=\"y\", alpha=0.25)\n",
        "plt.xticks(rotation=12)\n",
        "plt.tight_layout()\n",
        "plt.savefig(EXTRA_DIR/\"val_confidence_comparison.png\", dpi=160)\n",
        "plt.close()\n",
        "\n",
        "# (4) QUALITY DISTRIBUTIONS (from user inference CSV if available)\n",
        "qual_csv = Path(ROOT_OUT)/\"inference_user_comparison_wide.csv\"\n",
        "if qual_csv.exists():\n",
        "    qdf = pd.read_csv(qual_csv)\n",
        "\n",
        "    for col in [\"blur_var\",\"brightness\",\"contrast\",\"pct_underexposed\",\"pct_overexposed\"]:\n",
        "        if col not in qdf.columns:\n",
        "            continue\n",
        "        plt.figure(figsize=(7,4))\n",
        "        groups = [qdf.loc[qdf[\"quality\"]==q, col].dropna().values for q in [\"Good\",\"Usable\",\"Poor\"] if q in qdf[\"quality\"].unique()]\n",
        "        labels = [q for q in [\"Good\",\"Usable\",\"Poor\"] if q in qdf[\"quality\"].unique()]\n",
        "        if len(groups) >= 2:\n",
        "            plt.boxplot(groups, labels=labels)\n",
        "            plt.title(f\"Quality Distribution: {col} (User Uploads)\")\n",
        "            plt.ylabel(col)\n",
        "            plt.grid(axis=\"y\", alpha=0.25)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(EXTRA_DIR/f\"quality_boxplot_{col}.png\", dpi=160)\n",
        "            plt.close()\n",
        "\n",
        "    if \"quality\" in qdf.columns:\n",
        "        counts_q = qdf[\"quality\"].value_counts()\n",
        "        plt.figure(figsize=(6,4))\n",
        "        plt.bar(counts_q.index.astype(str), counts_q.values)\n",
        "        plt.title(\"User Uploads: Quality Label Distribution\")\n",
        "        plt.ylabel(\"Count\")\n",
        "        plt.grid(axis=\"y\", alpha=0.25)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(EXTRA_DIR/\"quality_distribution_counts.png\", dpi=160)\n",
        "        plt.close()\n",
        "else:\n",
        "    print(\"[extra] Quality plots skipped: run user inference (upload images) to create inference_user_comparison_wide.csv\")\n",
        "\n",
        "# (5) CASE STUDIES (annotate 3 validation images)\n",
        "case_dir = EXTRA_DIR/\"case_studies\"\n",
        "case_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "DF_VA_cases = DF_VA.copy()\n",
        "Xs = sc_r.transform(Xva_r)\n",
        "va_pred = clf_r.predict(Xs)\n",
        "va_conf = clf_r.predict_proba(Xs).max(axis=1) if hasattr(clf_r,\"predict_proba\") else np.ones(len(va_pred))\n",
        "\n",
        "DF_VA_cases[\"pred_resnet_linear\"] = va_pred\n",
        "DF_VA_cases[\"conf_resnet_linear\"] = va_conf\n",
        "DF_VA_cases[\"correct\"] = (DF_VA_cases[\"pred_resnet_linear\"].values == DF_VA_cases[\"level\"].values)\n",
        "\n",
        "c1 = DF_VA_cases[DF_VA_cases[\"correct\"]].sort_values(\"conf_resnet_linear\", ascending=False).head(1)\n",
        "c2 = DF_VA_cases[~DF_VA_cases[\"correct\"]].sort_values(\"conf_resnet_linear\", ascending=False).head(1)\n",
        "c3 = DF_VA_cases.sort_values(\"conf_resnet_linear\", ascending=True).head(1)\n",
        "picked = pd.concat([c1,c2,c3], ignore_index=True).drop_duplicates(subset=[\"proc_path\"]).head(3)\n",
        "\n",
        "for i, row in picked.iterrows():\n",
        "    imgp = row[\"proc_path\"]\n",
        "    true_y = int(row[\"level\"])\n",
        "    cr = float(row[\"conf_resnet_linear\"])\n",
        "\n",
        "    with Image.open(imgp) as im:\n",
        "        im = ImageOps.exif_transpose(im).convert(\"RGB\").resize((IMAGE_SIZE, IMAGE_SIZE))\n",
        "        arr = np.asarray(im, dtype=np.float32)/255.0\n",
        "    x = torch.from_numpy(np.transpose(arr,(2,0,1))).unsqueeze(0).to(device)\n",
        "    x = (x - MEAN.to(device)) / STD.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        fr = resnet(x); fv = vit(x); fa = ae_backbone(x)\n",
        "        if fr.dim()==4: fr = fr.mean((2,3))\n",
        "        if fv.dim()==4: fv = fv.mean((2,3))\n",
        "        if fa.dim()==4: fa = fa.mean((2,3))\n",
        "\n",
        "    pr2 = int(clf_r.predict(sc_r.transform(fr.cpu().numpy()))[0])\n",
        "    pv2 = int(clf_v.predict(sc_v.transform(fv.cpu().numpy()))[0])\n",
        "    pa2 = int(clf_ae.predict(sc_ae.transform(fa.cpu().numpy()))[0])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        ft_log = ft_model(x)\n",
        "        ft_p = torch.softmax(ft_log, dim=1).cpu().numpy()[0]\n",
        "        pft = int(np.argmax(ft_p))\n",
        "        cft = float(np.max(ft_p))\n",
        "\n",
        "    outp = case_dir / f\"case_{i+1}.jpg\"\n",
        "    lines = [\n",
        "        f\"TRUE={true_y} | ResNetLin={pr2} (conf~{cr:.2f})\",\n",
        "        f\"ViTLin={pv2} | AELin={pa2}\",\n",
        "        f\"ResNetFT={pft} (conf~{cft:.2f})\",\n",
        "    ]\n",
        "    annotate_case(imgp, lines, outp)\n",
        "\n",
        "# (6) WORKFLOW DIAGRAM (draw.io)\n",
        "diagram_path = EXTRA_DIR/\"workflow_diagram.drawio\"\n",
        "drawio_xml = \"\"\"<mxfile host=\"app.diagrams.net\">\n",
        "  <diagram name=\"DR Workflow\">\n",
        "    <mxGraphModel dx=\"1000\" dy=\"700\" grid=\"1\" gridSize=\"10\" guides=\"1\" tooltips=\"1\" connect=\"1\" arrows=\"1\" fold=\"1\" page=\"1\" pageScale=\"1\" pageWidth=\"1100\" pageHeight=\"850\">\n",
        "      <root>\n",
        "        <mxCell id=\"0\"/><mxCell id=\"1\" parent=\"0\"/>\n",
        "        <mxCell id=\"2\" value=\"Input Data&#10;(train_subset.zip + trainLabels.csv)\" style=\"rounded=1;whiteSpace=wrap;html=1;\" vertex=\"1\" parent=\"1\"><mxGeometry x=\"40\" y=\"80\" width=\"240\" height=\"70\" as=\"geometry\"/></mxCell>\n",
        "        <mxCell id=\"3\" value=\"Preprocess&#10;crop_black + autocontrast + equalize&#10;center-crop + resize\" style=\"rounded=1;whiteSpace=wrap;html=1;\" vertex=\"1\" parent=\"1\"><mxGeometry x=\"320\" y=\"80\" width=\"260\" height=\"90\" as=\"geometry\"/></mxCell>\n",
        "        <mxCell id=\"4\" value=\"Stratified Split&#10;train/val\" style=\"rounded=1;whiteSpace=wrap;html=1;\" vertex=\"1\" parent=\"1\"><mxGeometry x=\"620\" y=\"90\" width=\"180\" height=\"60\" as=\"geometry\"/></mxCell>\n",
        "        <mxCell id=\"5\" value=\"Feature Backbones&#10;ResNet50 (ImageNet)&#10;ViT-B/16 (ImageNet)&#10;Improved AE Encoder\" style=\"rounded=1;whiteSpace=wrap;html=1;\" vertex=\"1\" parent=\"1\"><mxGeometry x=\"40\" y=\"240\" width=\"280\" height=\"110\" as=\"geometry\"/></mxCell>\n",
        "        <mxCell id=\"6\" value=\"Linear Head&#10;StandardScaler + LogisticRegression\" style=\"rounded=1;whiteSpace=wrap;html=1;\" vertex=\"1\" parent=\"1\"><mxGeometry x=\"360\" y=\"250\" width=\"260\" height=\"80\" as=\"geometry\"/></mxCell>\n",
        "        <mxCell id=\"7\" value=\"Evaluation&#10;Accuracy + QWK&#10;Confusion Matrix + ROC\" style=\"rounded=1;whiteSpace=wrap;html=1;\" vertex=\"1\" parent=\"1\"><mxGeometry x=\"660\" y=\"250\" width=\"240\" height=\"90\" as=\"geometry\"/></mxCell>\n",
        "        <mxCell id=\"8\" value=\"Fine-tune ResNet50&#10;(Stage 1: partial unfreeze&#10;Stage 2: full unfreeze + early stop)\" style=\"rounded=1;whiteSpace=wrap;html=1;\" vertex=\"1\" parent=\"1\"><mxGeometry x=\"40\" y=\"410\" width=\"320\" height=\"100\" as=\"geometry\"/></mxCell>\n",
        "        <mxCell id=\"9\" value=\"User Inference&#10;3-model predictions + confidence\" style=\"rounded=1;whiteSpace=wrap;html=1;\" vertex=\"1\" parent=\"1\"><mxGeometry x=\"400\" y=\"420\" width=\"240\" height=\"80\" as=\"geometry\"/></mxCell>\n",
        "        <mxCell id=\"10\" value=\"Quality Metrics&#10;blur_var + brightness + exposure%\" style=\"rounded=1;whiteSpace=wrap;html=1;\" vertex=\"1\" parent=\"1\"><mxGeometry x=\"680\" y=\"420\" width=\"240\" height=\"80\" as=\"geometry\"/></mxCell>\n",
        "        <mxCell id=\"11\" value=\"Consensus + All-Model Risk&#10;Majority vote + unanimous check\" style=\"rounded=1;whiteSpace=wrap;html=1;\" vertex=\"1\" parent=\"1\"><mxGeometry x=\"940\" y=\"420\" width=\"240\" height=\"80\" as=\"geometry\"/></mxCell>\n",
        "        <mxCell id=\"e1\" style=\"endArrow=block;html=1;\" edge=\"1\" parent=\"1\" source=\"2\" target=\"3\"><mxGeometry relative=\"1\" as=\"geometry\"/></mxCell>\n",
        "        <mxCell id=\"e2\" style=\"endArrow=block;html=1;\" edge=\"1\" parent=\"1\" source=\"3\" target=\"4\"><mxGeometry relative=\"1\" as=\"geometry\"/></mxCell>\n",
        "        <mxCell id=\"e3\" style=\"endArrow=block;html=1;\" edge=\"1\" parent=\"1\" source=\"4\" target=\"5\"><mxGeometry relative=\"1\" as=\"geometry\"/></mxCell>\n",
        "        <mxCell id=\"e4\" style=\"endArrow=block;html=1;\" edge=\"1\" parent=\"1\" source=\"5\" target=\"6\"><mxGeometry relative=\"1\" as=\"geometry\"/></mxCell>\n",
        "        <mxCell id=\"e5\" style=\"endArrow=block;html=1;\" edge=\"1\" parent=\"1\" source=\"6\" target=\"7\"><mxGeometry relative=\"1\" as=\"geometry\"/></mxCell>\n",
        "        <mxCell id=\"e6\" style=\"endArrow=block;html=1;\" edge=\"1\" parent=\"1\" source=\"7\" target=\"8\"><mxGeometry relative=\"1\" as=\"geometry\"/></mxCell>\n",
        "        <mxCell id=\"e7\" style=\"endArrow=block;html=1;\" edge=\"1\" parent=\"1\" source=\"8\" target=\"9\"><mxGeometry relative=\"1\" as=\"geometry\"/></mxCell>\n",
        "        <mxCell id=\"e8\" style=\"endArrow=block;html=1;\" edge=\"1\" parent=\"1\" source=\"9\" target=\"10\"><mxGeometry relative=\"1\" as=\"geometry\"/></mxCell>\n",
        "        <mxCell id=\"e9\" style=\"endArrow=block;html=1;\" edge=\"1\" parent=\"1\" source=\"10\" target=\"11\"><mxGeometry relative=\"1\" as=\"geometry\"/></mxCell>\n",
        "      </root>\n",
        "    </mxGraphModel>\n",
        "  </diagram>\n",
        "</mxfile>\n",
        "\"\"\"\n",
        "with open(diagram_path, \"w\") as f:\n",
        "    f.write(drawio_xml)\n",
        "\n",
        "# (7) REDUCTION IMPACT (from user inference CSV)\n",
        "if qual_csv.exists():\n",
        "    qdf = pd.read_csv(qual_csv)\n",
        "    model_names = [\"resnet50_linear\",\"vit_b16_linear\",\"autoencoder_linear\"]\n",
        "    rates = []\n",
        "    for m in model_names:\n",
        "        c = f\"risk@{m}\"\n",
        "        if c in qdf.columns:\n",
        "            rates.append((m, float((qdf[c] == \"Yes\").mean())))\n",
        "    if \"consensus_risk\" in qdf.columns:\n",
        "        rates.append((\"consensus_risk\", float((qdf[\"consensus_risk\"] == \"Yes\").mean())))\n",
        "    if \"all_models_risk\" in qdf.columns:\n",
        "        rates.append((\"all_models_risk\", float((qdf[\"all_models_risk\"] == \"Yes\").mean())))\n",
        "\n",
        "    red = pd.DataFrame(rates, columns=[\"system\",\"fraction_flagged_dr\"])\n",
        "    red.to_csv(EXTRA_DIR/\"reduction_impact.csv\", index=False)\n",
        "\n",
        "    plt.figure(figsize=(7.4,4))\n",
        "    plt.bar(red[\"system\"], red[\"fraction_flagged_dr\"])\n",
        "    plt.ylim(0,1)\n",
        "    plt.ylabel(\"Fraction flagged DR\")\n",
        "    plt.title(\"Reduction Impact: Single Models vs Consensus vs Unanimous\")\n",
        "    plt.grid(axis=\"y\", alpha=0.25)\n",
        "    plt.xticks(rotation=12)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(EXTRA_DIR/\"reduction_impact.png\", dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "# Copy extras into public folder\n",
        "for p in EXTRA_DIR.glob(\"**/*\"):\n",
        "    if p.is_file():\n",
        "        relname = p.name if p.parent == EXTRA_DIR else str(p.relative_to(EXTRA_DIR)).replace(\"/\", \"_\")\n",
        "        shutil.copy2(str(p), str(Path(FILES_DIR)/relname))\n",
        "\n",
        "print(\"[extra] Extra artifacts saved to:\", EXTRA_DIR)\n",
        "print(\"[extra] Extra artifacts copied to public folder:\", FILES_DIR)\n",
        "# ==========================================================================================\n",
        "\n",
        "# ------------------ Predict helpers ------------------\n",
        "def _to_gray_224(path, size=IMAGE_SIZE):\n",
        "    with Image.open(path) as im:\n",
        "        g = ImageOps.exif_transpose(im).convert(\"L\").resize((size,size))\n",
        "        return np.asarray(g, dtype=np.float32)\n",
        "def _conv2(img, k):\n",
        "    kh, kw = k.shape; ph, pw = kh//2, kw//2\n",
        "    pad = np.pad(img, ((ph,ph),(pw,pw)), mode=\"reflect\")\n",
        "    H,W = img.shape; out = np.empty_like(img, dtype=np.float32)\n",
        "    for i in range(H):\n",
        "        for j in range(W):\n",
        "            out[i,j] = float((pad[i:i+kh, j:j+kw]*k).sum())\n",
        "    return out\n",
        "def quality_metrics(path):\n",
        "    try: g = _to_gray_224(path)\n",
        "    except Exception:\n",
        "        return dict(quality=\"Poor\", blur_var=0.0, brightness=0.0, contrast=0.0,\n",
        "                    pct_underexposed=1.0, pct_overexposed=0.0)\n",
        "    brightness=float(g.mean()); contrast=float(g.std())\n",
        "    lap=np.array([[0,1,0],[1,-4,1],[0,1,0]],dtype=np.float32)\n",
        "    blur_var=float(_conv2(g,lap).var())\n",
        "    pct_under=float((g<25).mean()); pct_over=float((g>230).mean())\n",
        "    if brightness<35 or blur_var<50 or pct_under>0.30 or pct_over>0.30: quality=\"Poor\"\n",
        "    elif brightness<60 or blur_var<120 or pct_under>0.10 or pct_over>0.10: quality=\"Usable\"\n",
        "    else: quality=\"Good\"\n",
        "    return dict(quality=quality, blur_var=blur_var, brightness=brightness,\n",
        "                contrast=contrast, pct_underexposed=pct_under, pct_overexposed=pct_over)\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_with(backbone, scaler, clf, img_paths, model_name):\n",
        "    rows=[]\n",
        "    for p in img_paths:\n",
        "        with Image.open(p) as im:\n",
        "            im = ImageOps.exif_transpose(im).convert(\"RGB\").resize((IMAGE_SIZE,IMAGE_SIZE))\n",
        "            arr = np.asarray(im, dtype=np.float32)/255.0\n",
        "        x = torch.from_numpy(np.transpose(arr,(2,0,1))).unsqueeze(0).to(device)\n",
        "        x = (x - MEAN.to(device)) / STD.to(device)\n",
        "        out = backbone(x)\n",
        "        if out.dim()==4: out = out.mean((2,3))\n",
        "        X = out.cpu().numpy()\n",
        "        Xs = scaler.transform(X)\n",
        "        pred = int(clf.predict(Xs)[0])\n",
        "        conf = float(clf.predict_proba(Xs).max(axis=1)[0]) if hasattr(clf,\"predict_proba\") else 1.0\n",
        "        risk = \"Yes\" if (pred>=1 if not BINARY_MODE else pred==1) else \"No\"\n",
        "        q = quality_metrics(p)\n",
        "        rows.append({\n",
        "            \"file\": Path(p).name, \"model_name\": model_name,\n",
        "            \"pred_level\": pred, \"risk\": risk, \"confidence\": conf,\n",
        "            \"quality\": q[\"quality\"], \"blur_var\": q[\"blur_var\"],\n",
        "            \"brightness\": q[\"brightness\"], \"contrast\": q[\"contrast\"],\n",
        "            \"pct_underexposed\": q[\"pct_underexposed\"], \"pct_overexposed\": q[\"pct_overexposed\"],\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "def run_comparative_inference_on_paths(paths):\n",
        "    exts={\".jpg\",\".jpeg\",\".png\",\".tif\",\".tiff\",\".bmp\"}\n",
        "    norm=[str(Path(p).resolve()) for p in paths if Path(p).is_file() and Path(p).suffix.lower() in exts]\n",
        "    norm = sorted(list(dict.fromkeys(norm)))\n",
        "    if not norm:\n",
        "        print(\"[inference] No valid image paths.\"); return None, None\n",
        "\n",
        "    print(f\"[inference] Predicting {len(norm)} images across 3 models...\")\n",
        "    dfs=[]\n",
        "    dfs.append(predict_with(resnet, sc_r, clf_r, norm, \"resnet50_linear\"))\n",
        "    dfs.append(predict_with(vit,    sc_v, clf_v, norm, \"vit_b16_linear\"))\n",
        "    dfs.append(predict_with(ae_backbone, sc_ae, clf_ae, norm, \"autoencoder_linear\"))\n",
        "\n",
        "    df_long = pd.concat(dfs, ignore_index=True)\n",
        "    piv_pred=df_long.pivot(index=\"file\", columns=\"model_name\", values=\"pred_level\")\n",
        "    piv_risk=df_long.pivot(index=\"file\", columns=\"model_name\", values=\"risk\")\n",
        "    piv_conf=df_long.pivot(index=\"file\", columns=\"model_name\", values=\"confidence\")\n",
        "    piv_pred.columns=[f\"pred_level@{c}\" for c in piv_pred.columns]\n",
        "    piv_risk.columns=[f\"risk@{c}\" for c in piv_risk.columns]\n",
        "    piv_conf.columns=[f\"confidence@{c}\" for c in piv_conf.columns]\n",
        "    qcols=[\"quality\",\"blur_var\",\"brightness\",\"contrast\",\"pct_underexposed\",\"pct_overexposed\"]\n",
        "    qual=(df_long.sort_values([\"file\",\"model_name\"]).drop_duplicates(\"file\")[[\"file\"]+qcols]).set_index(\"file\")\n",
        "    df_wide=qual.join([piv_pred,piv_risk,piv_conf], how=\"left\").reset_index()\n",
        "\n",
        "    lvl_cols=[c for c in df_wide.columns if c.startswith(\"pred_level@\")]\n",
        "    risk_cols=[c for c in df_wide.columns if c.startswith(\"risk@\")]\n",
        "    df_wide[\"consensus_level\"]=df_wide[lvl_cols].mode(axis=1)[0]\n",
        "    df_wide[\"consensus_risk\"]=df_wide[risk_cols].mode(axis=1)[0]\n",
        "\n",
        "    model_names = [\"resnet50_linear\",\"vit_b16_linear\",\"autoencoder_linear\"]\n",
        "    all_risk_cols = [f\"risk@{m}\" for m in model_names if f\"risk@{m}\" in df_wide.columns]\n",
        "    def all_yes(row):\n",
        "        vals = [row[c] for c in all_risk_cols if c in row and pd.notna(row[c])]\n",
        "        return \"Yes\" if (len(vals) == len(all_risk_cols) and all(v==\"Yes\" for v in vals)) else \"No\"\n",
        "    df_wide[\"all_models_risk\"] = df_wide.apply(all_yes, axis=1)\n",
        "\n",
        "    out_long=Path(ROOT_OUT)/\"inference_user_predictions_long.csv\"\n",
        "    out_wide=Path(ROOT_OUT)/\"inference_user_comparison_wide.csv\"\n",
        "    df_long.to_csv(out_long, index=False); df_wide.to_csv(out_wide, index=False)\n",
        "    print(\"Saved:\", out_long, \"\\nSaved:\", out_wide)\n",
        "\n",
        "    out_allrisk = Path(ROOT_OUT)/\"inference_only_all_models_risk_yes.csv\"\n",
        "    df_wide[df_wide[\"all_models_risk\"]==\"Yes\"].to_csv(out_allrisk, index=False)\n",
        "    print(\"Saved:\", out_allrisk)\n",
        "\n",
        "    # Plots\n",
        "    conf = df_long.groupby(\"model_name\", dropna=False)[\"confidence\"].mean().reset_index()\n",
        "    plt.figure(figsize=(7,4)); order = conf.sort_values(\"confidence\", ascending=False)\n",
        "    plt.bar(order[\"model_name\"], order[\"confidence\"]); plt.ylim(0,1)\n",
        "    plt.title(\"User Inference: Mean Confidence per Model\"); plt.ylabel(\"Mean confidence\"); plt.grid(axis=\"y\", alpha=0.25); plt.xticks(rotation=12)\n",
        "    plt.tight_layout(); plt.savefig(Path(ROOT_OUT)/\"plot_user_confidence_per_model.png\", dpi=140); plt.close()\n",
        "\n",
        "    tmp = df_long.copy()\n",
        "    tmp[\"is_dr\"] = (tmp[\"pred_level\"]>=1).astype(float) if not BINARY_MODE else (tmp[\"pred_level\"]==1).astype(float)\n",
        "    dr_rate = tmp.groupby(\"model_name\", dropna=False)[\"is_dr\"].mean().reset_index()\n",
        "    plt.figure(figsize=(7,4)); order = dr_rate.sort_values(\"is_dr\", ascending=False)\n",
        "    plt.bar(order[\"model_name\"], order[\"is_dr\"]); plt.ylim(0,1)\n",
        "    ttl = \"User Inference: % Images Flagged DR\" if BINARY_MODE else \"User Inference: % Images Level ≥ 1\"\n",
        "    plt.title(ttl); plt.ylabel(\"Fraction\"); plt.grid(axis=\"y\", alpha=0.25); plt.xticks(rotation=12)\n",
        "    plt.tight_layout(); plt.savefig(Path(ROOT_OUT)/\"plot_user_dr_rate_per_model.png\", dpi=140); plt.close()\n",
        "\n",
        "    return df_long, df_wide\n",
        "\n",
        "# ------------------ Upload images & run batch inference ------------------\n",
        "print(\"\\n=== Upload images for batch inference (optional) ===\")\n",
        "user_paths=[]\n",
        "try:\n",
        "    from google.colab import files\n",
        "    uploads = files.upload()\n",
        "    for name, data in uploads.items():\n",
        "        dst = Path(ROOT_OUT)/f\"uploaded_{name}\"\n",
        "        with open(dst,\"wb\") as f: f.write(data)\n",
        "        user_paths.append(str(dst))\n",
        "except Exception:\n",
        "    print(\"[info] Upload skipped.\")\n",
        "\n",
        "if user_paths:\n",
        "    df_long, df_wide = run_comparative_inference_on_paths(user_paths)\n",
        "    def _img_to_base64(path):\n",
        "        try:\n",
        "            with open(path, \"rb\") as f:\n",
        "                return \"data:image/png;base64,\" + base64.b64encode(f.read()).decode(\"utf-8\")\n",
        "        except: return \"\"\n",
        "    fn2path = {Path(p).name: p for p in user_paths}\n",
        "    secs=[]\n",
        "    for _,row in df_wide.iterrows():\n",
        "        fn=row[\"file\"]; src=fn2path.get(fn,\"\"); b64=_img_to_base64(src) if src else \"\"\n",
        "        item=f\"\"\"\n",
        "        <section style=\"border:1px solid #ddd;border-radius:10px;padding:12px;margin:12px;\">\n",
        "          <h3 style=\"margin:0 0 8px 0;\">{fn}</h3>\n",
        "          <img src=\"{b64}\" style=\"width:240px;border:1px solid #eee;border-radius:8px\"/>\n",
        "          <div>Consensus level: <b>{row['consensus_level']}</b> |\n",
        "               Consensus risk: <b>{row['consensus_risk']}</b> |\n",
        "               All models risk: <b>{row['all_models_risk']}</b></div>\n",
        "        </section>\"\"\"\n",
        "        secs.append(item)\n",
        "    html=f\"<html><body><h2>DR Batch Report</h2>{''.join(secs)}</body></html>\"\n",
        "    with open(Path(ROOT_OUT)/\"report_xai.html\",\"w\") as f: f.write(html)\n",
        "else:\n",
        "    print(\"[batch] No images uploaded; skipping batch inference/report.\")\n",
        "\n",
        "# ------------------ Copy artifacts to static + download ------------------\n",
        "def copy_artifacts_to_static():\n",
        "    to_copy = [\n",
        "        Path(ROOT_OUT)/\"inference_user_predictions_long.csv\",\n",
        "        Path(ROOT_OUT)/\"inference_user_comparison_wide.csv\",\n",
        "        Path(ROOT_OUT)/\"inference_only_all_models_risk_yes.csv\",\n",
        "        Path(ROOT_OUT)/\"summary_models.csv\",\n",
        "        Path(ROOT_OUT)/\"acc_by_model.png\",\n",
        "        Path(ROOT_OUT)/\"qwk_by_model.png\",\n",
        "        Path(ROOT_OUT)/\"plot_user_confidence_per_model.png\",\n",
        "        Path(ROOT_OUT)/\"plot_user_dr_rate_per_model.png\",\n",
        "        Path(ROOT_OUT)/\"report_xai.html\",\n",
        "    ]\n",
        "    copied=[]\n",
        "    for p in to_copy:\n",
        "        if Path(p).exists():\n",
        "            dst = Path(FILES_DIR)/Path(p).name\n",
        "            shutil.copy2(str(p), str(dst))\n",
        "            copied.append(str(dst))\n",
        "    print(\"\\nArtifacts in /api/files/:\")\n",
        "    for c in copied: print(\" -\", Path(c).name)\n",
        "    return copied\n",
        "\n",
        "copied = copy_artifacts_to_static()\n",
        "if AUTO_DOWNLOAD:\n",
        "    try:\n",
        "        from google.colab import files as _files\n",
        "        for fp in copied:\n",
        "            print(\"[download]\", Path(fp).name)\n",
        "            _files.download(fp)\n",
        "            time.sleep(0.2)\n",
        "    except Exception as e:\n",
        "        print(\"[download] skipped:\", e)\n",
        "\n",
        "# ------------------ Persist models for Cell 2 ------------------\n",
        "torch.save(ae_model.state_dict(), str(Path(MODELS_DIR)/\"ae_model.pt\"))\n",
        "joblib.dump(sc_r,  str(Path(MODELS_DIR)/\"resnet_scaler.joblib\"))\n",
        "joblib.dump(clf_r, str(Path(MODELS_DIR)/\"resnet_clf.joblib\"))\n",
        "joblib.dump(sc_v,  str(Path(MODELS_DIR)/\"vit_scaler.joblib\"))\n",
        "joblib.dump(clf_v, str(Path(MODELS_DIR)/\"vit_clf.joblib\"))\n",
        "joblib.dump(sc_ae, str(Path(MODELS_DIR)/\"ae_scaler.joblib\"))\n",
        "joblib.dump(clf_ae,str(Path(MODELS_DIR)/\"ae_clf.joblib\"))\n",
        "\n",
        "# ✅ Save fine-tuned ResNet50 checkpoint (fixes Cell 2 FileNotFoundError)\n",
        "torch.save({\"state_dict\": ft_model.state_dict(), \"arch\": \"resnet50\", \"num_classes\": num_classes},\n",
        "           str(Path(MODELS_DIR)/\"resnet50_ft.pt\"))\n",
        "\n",
        "# Save config so Cell 2 can load seamlessly\n",
        "cfg = {\n",
        "    \"BINARY_MODE\": BINARY_MODE,\n",
        "    \"IMAGE_SIZE\": IMAGE_SIZE,\n",
        "    \"ROOT_OUT\": ROOT_OUT,\n",
        "    \"FILES_DIR\": FILES_DIR,\n",
        "    \"MODELS_DIR\": MODELS_DIR,\n",
        "    \"MEAN\": [0.485,0.456,0.406],\n",
        "    \"STD\":  [0.229,0.224,0.225],\n",
        "}\n",
        "with open(Path(MODELS_DIR)/\"ui_config.json\",\"w\") as f: json.dump(cfg, f, indent=2)\n",
        "\n",
        "print(\"\\n✅ Cell 1 finished. Models & artifacts saved.\")\n",
        "print(\"➡️ Now run **Cell 2** to launch the Gradio UI.\")\n",
        "\n",
        "# ==========================================================================================\n",
        "# ===================== EXTRA RESULTS: CM + ROC + CONF + CASES + DIAGRAM + REDUCTION\n",
        "# ==========================================================================================\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "EXTRA_DIR = Path(ROOT_OUT) / \"extra_results\"\n",
        "EXTRA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def save_confusion_matrix(y_true, y_pred, labels, title, out_png):\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(5.8, 5.0))\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "    disp.plot(ax=ax, cmap=None, colorbar=True, values_format=\"d\")\n",
        "    ax.set_title(title)\n",
        "    plt.tight_layout()\n",
        "    fig.savefig(out_png, dpi=180)\n",
        "    plt.close(fig)\n",
        "    return cm\n",
        "\n",
        "def save_roc_curve_binary(y_true_bin, y_score, title, out_png):\n",
        "    fpr, tpr, _ = roc_curve(y_true_bin, y_score)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6.0, 4.8))\n",
        "    ax.plot(fpr, tpr, label=f\"AUC = {roc_auc:.3f}\")\n",
        "    ax.plot([0,1], [0,1], linestyle=\"--\", label=\"Chance\")\n",
        "    ax.set_xlim(0,1); ax.set_ylim(0,1.02)\n",
        "    ax.set_xlabel(\"False Positive Rate\")\n",
        "    ax.set_ylabel(\"True Positive Rate\")\n",
        "    ax.set_title(title)\n",
        "    ax.grid(alpha=0.25)\n",
        "    ax.legend(loc=\"lower right\")\n",
        "    plt.tight_layout()\n",
        "    fig.savefig(out_png, dpi=180)\n",
        "    plt.close(fig)\n",
        "    return roc_auc\n",
        "\n",
        "def safe_predict_linear(Xva, scaler, clf):\n",
        "    Xs = scaler.transform(Xva)\n",
        "    pred = clf.predict(Xs)\n",
        "    proba = None\n",
        "    if hasattr(clf, \"predict_proba\"):\n",
        "        try:\n",
        "            proba = clf.predict_proba(Xs)\n",
        "        except Exception:\n",
        "            proba = None\n",
        "    return pred, proba\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_ft_probs(model, loader):\n",
        "    model.eval()\n",
        "    probs, preds, ys = [], [], []\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(device)\n",
        "        logits = model(xb)\n",
        "        p = torch.softmax(logits, dim=1)\n",
        "        probs.append(p.cpu().numpy())\n",
        "        preds.append(torch.argmax(p, dim=1).cpu().numpy())\n",
        "        ys.append(np.asarray(yb))\n",
        "    return np.concatenate(probs), np.concatenate(preds), np.concatenate(ys)\n",
        "\n",
        "# -------- (1) Compute VAL predictions for all models --------\n",
        "labels_list = [0,1] if BINARY_MODE else list(range(num_classes))\n",
        "\n",
        "pred_r, proba_r = safe_predict_linear(Xva_r, sc_r, clf_r)\n",
        "pred_v, proba_v = safe_predict_linear(Xva_v, sc_v, clf_v)\n",
        "pred_ae, proba_ae = safe_predict_linear(Xva_ae, sc_ae, clf_ae)\n",
        "\n",
        "# Fine-tuned predictions (always probabilities via softmax)\n",
        "ft_probs, ft_pred, ft_y = predict_ft_probs(ft_model, VA_LOADER)\n",
        "\n",
        "# -------- (2) Confusion matrices (always saved) --------\n",
        "save_confusion_matrix(yva, pred_r, labels_list,\n",
        "                      \"Confusion Matrix — ResNet50 Linear (Val)\",\n",
        "                      EXTRA_DIR / \"cm_resnet50_linear.png\")\n",
        "\n",
        "save_confusion_matrix(yva, pred_v, labels_list,\n",
        "                      \"Confusion Matrix — ViT-B/16 Linear (Val)\",\n",
        "                      EXTRA_DIR / \"cm_vit_b16_linear.png\")\n",
        "\n",
        "save_confusion_matrix(yva, pred_ae, labels_list,\n",
        "                      \"Confusion Matrix — Autoencoder Linear (Val)\",\n",
        "                      EXTRA_DIR / \"cm_autoencoder_linear.png\")\n",
        "\n",
        "save_confusion_matrix(ft_y, ft_pred, labels_list,\n",
        "                      \"Confusion Matrix — ResNet50 Fine-tuned (Val)\",\n",
        "                      EXTRA_DIR / \"cm_resnet50_finetune.png\")\n",
        "\n",
        "print(\"[extra] ✅ Confusion matrices saved to:\", EXTRA_DIR)\n",
        "\n",
        "# -------- (3) ROC curves (binary only, robust checks) --------\n",
        "if BINARY_MODE:\n",
        "    y_true_bin = (yva == 1).astype(int)\n",
        "\n",
        "    if proba_r is not None:\n",
        "        auc_r = save_roc_curve_binary(y_true_bin, proba_r[:,1],\n",
        "                                      \"ROC — ResNet50 Linear (Val)\",\n",
        "                                      EXTRA_DIR / \"roc_resnet50_linear.png\")\n",
        "    else:\n",
        "        auc_r = None\n",
        "\n",
        "    if proba_v is not None:\n",
        "        auc_v = save_roc_curve_binary(y_true_bin, proba_v[:,1],\n",
        "                                      \"ROC — ViT-B/16 Linear (Val)\",\n",
        "                                      EXTRA_DIR / \"roc_vit_b16_linear.png\")\n",
        "    else:\n",
        "        auc_v = None\n",
        "\n",
        "    if proba_ae is not None:\n",
        "        auc_ae = save_roc_curve_binary(y_true_bin, proba_ae[:,1],\n",
        "                                       \"ROC — Autoencoder Linear (Val)\",\n",
        "                                       EXTRA_DIR / \"roc_autoencoder_linear.png\")\n",
        "    else:\n",
        "        auc_ae = None\n",
        "\n",
        "    auc_ft = save_roc_curve_binary((ft_y==1).astype(int), ft_probs[:,1],\n",
        "                                   \"ROC — ResNet50 Fine-tuned (Val)\",\n",
        "                                   EXTRA_DIR / \"roc_resnet50_finetune.png\")\n",
        "\n",
        "    print(\"[extra] ✅ ROC saved. AUCs:\",\n",
        "          {\"resnet_lin\": auc_r, \"vit_lin\": auc_v, \"ae_lin\": auc_ae, \"resnet_ft\": auc_ft})\n",
        "else:\n",
        "    print(\"[extra] ROC skipped (BINARY_MODE=False).\")\n",
        "\n",
        "# -------- (4) Confidence comparison --------\n",
        "def mean_max_conf(proba):\n",
        "    return float(np.max(proba, axis=1).mean())\n",
        "\n",
        "conf_rows = []\n",
        "if proba_r is not None: conf_rows.append((\"resnet50_linear\", mean_max_conf(proba_r)))\n",
        "if proba_v is not None: conf_rows.append((\"vit_b16_linear\", mean_max_conf(proba_v)))\n",
        "if proba_ae is not None: conf_rows.append((\"autoencoder_linear\", mean_max_conf(proba_ae)))\n",
        "conf_rows.append((\"resnet50_finetune\", float(np.max(ft_probs, axis=1).mean())))\n",
        "\n",
        "val_conf = pd.DataFrame(conf_rows, columns=[\"model\",\"mean_max_conf\"])\n",
        "val_conf.to_csv(EXTRA_DIR/\"val_confidence_comparison.csv\", index=False)\n",
        "\n",
        "plt.figure(figsize=(7.2,4.2))\n",
        "plt.bar(val_conf[\"model\"], val_conf[\"mean_max_conf\"])\n",
        "plt.ylim(0,1)\n",
        "plt.title(\"Validation: Mean Confidence (Max Prob)\")\n",
        "plt.ylabel(\"Mean max probability\")\n",
        "plt.grid(axis=\"y\", alpha=0.25)\n",
        "plt.xticks(rotation=12)\n",
        "plt.tight_layout()\n",
        "plt.savefig(EXTRA_DIR/\"val_confidence_comparison.png\", dpi=180)\n",
        "plt.close()\n",
        "\n",
        "# -------- (5) Always list what actually exists (so you can confirm) --------\n",
        "print(\"\\n[extra] ✅ Files created in extra_results:\")\n",
        "for p in sorted(EXTRA_DIR.glob(\"*.png\")):\n",
        "    print(\" -\", p.name)\n",
        "for p in sorted(EXTRA_DIR.glob(\"*.csv\")):\n",
        "    print(\" -\", p.name)\n",
        "\n",
        "# -------- (6) Copy extras into public folder (your /api/files/) --------\n",
        "for p in EXTRA_DIR.glob(\"*\"):\n",
        "    if p.is_file():\n",
        "        shutil.copy2(str(p), str(Path(FILES_DIR)/p.name))\n",
        "\n",
        "print(\"\\n[extra] ✅ Copied extra_results to public folder:\", FILES_DIR)\n",
        "# ==========================================================================================\n",
        "\n",
        "\n",
        "from IPython.display import Image as IPyImage, display\n",
        "display(IPyImage(str(EXTRA_DIR/\"cm_resnet50_linear.png\")))\n",
        "display(IPyImage(str(EXTRA_DIR/\"roc_resnet50_linear.png\")))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === DR — CELL 2 (One-and-done UI): load, predict once, auto-terminate ===\n",
        "import os, json, time, threading, sys\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from PIL import Image, ImageOps\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "import joblib\n",
        "\n",
        "# ---------- Load config & common ----------\n",
        "MODELS_DIR = \"/content/dr_one_cell/models\"\n",
        "with open(Path(MODELS_DIR)/\"ui_config.json\") as f:\n",
        "    cfg = json.load(f)\n",
        "\n",
        "ROOT_OUT     = cfg[\"ROOT_OUT\"]\n",
        "FILES_DIR    = cfg[\"FILES_DIR\"]\n",
        "BINARY_MODE  = bool(cfg[\"BINARY_MODE\"])\n",
        "IMAGE_SIZE   = int(cfg[\"IMAGE_SIZE\"])\n",
        "MEAN         = torch.tensor(cfg[\"MEAN\"]).view(3,1,1)\n",
        "STD          = torch.tensor(cfg[\"STD\"]).view(3,1,1)\n",
        "device       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ---------- Backbones ----------\n",
        "def build_resnet50_feats():\n",
        "    try:\n",
        "        m = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
        "    except Exception:\n",
        "        m = models.resnet50(weights=None)\n",
        "    m.fc = nn.Identity(); m.eval(); return m.to(device)\n",
        "\n",
        "def build_vit_b16_feats():\n",
        "    try:\n",
        "        m = models.vit_b_16(weights=models.ViT_B_16_Weights.IMAGENET1K_V1)\n",
        "    except Exception:\n",
        "        m = models.vit_b_16(weights=None)\n",
        "    m.heads = nn.Identity(); m.eval(); return m.to(device)\n",
        "\n",
        "# Match Cell 1 AE encoder exactly\n",
        "class ImprovedAE(nn.Module):\n",
        "    def __init__(self, latent_dim=384):\n",
        "        super().__init__()\n",
        "        ch = [3, 64, 128, 256, 512]\n",
        "        enc=[]\n",
        "        for i in range(len(ch)-1):\n",
        "            enc += [nn.Conv2d(ch[i], ch[i+1], 3, stride=2, padding=1, bias=False),\n",
        "                    nn.BatchNorm2d(ch[i+1]), nn.GELU()]\n",
        "        self.enc_conv = nn.Sequential(*enc)\n",
        "        self.enc_gap  = nn.AdaptiveAvgPool2d(1)\n",
        "        self.enc_fc   = nn.Linear(512, latent_dim)\n",
        "        # decoder (present in checkpoint)\n",
        "        self.dec_fc   = nn.Linear(latent_dim, 512*14*14)\n",
        "        self.dec_deconv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(512, 256, 3, stride=2, padding=1, output_padding=1), nn.BatchNorm2d(256), nn.GELU(),\n",
        "            nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1), nn.BatchNorm2d(128), nn.GELU(),\n",
        "            nn.ConvTranspose2d(128,  64, 3, stride=2, padding=1, output_padding=1), nn.BatchNorm2d(64),  nn.GELU(),\n",
        "            nn.ConvTranspose2d(64,     3, 3, stride=2, padding=1, output_padding=1),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        h = self.enc_conv(x); h = self.enc_gap(h).flatten(1); return self.enc_fc(h)\n",
        "\n",
        "def build_ae_feats_from_ckpt(ckpt_path, latent_dim=384):\n",
        "    base = ImprovedAE(latent_dim=latent_dim)\n",
        "    sd = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "    base.load_state_dict(sd, strict=True)\n",
        "    class EncOnly(nn.Module):\n",
        "        def __init__(self, ae):\n",
        "            super().__init__(); self.enc_conv=ae.enc_conv; self.enc_gap=ae.enc_gap; self.enc_fc=ae.enc_fc\n",
        "        def forward(self, x):\n",
        "            h=self.enc_conv(x); h=self.enc_gap(h).flatten(1); return self.enc_fc(h)\n",
        "    return EncOnly(base).eval().to(device)\n",
        "\n",
        "# Feature extractors (linear heads)\n",
        "resnet = build_resnet50_feats()\n",
        "vit    = build_vit_b16_feats()\n",
        "ae_backbone = build_ae_feats_from_ckpt(Path(MODELS_DIR)/\"ae_model.pt\", latent_dim=384)\n",
        "\n",
        "sc_r   = joblib.load(Path(MODELS_DIR)/\"resnet_scaler.joblib\")\n",
        "clf_r  = joblib.load(Path(MODELS_DIR)/\"resnet_clf.joblib\")\n",
        "sc_v   = joblib.load(Path(MODELS_DIR)/\"vit_scaler.joblib\")\n",
        "clf_v  = joblib.load(Path(MODELS_DIR)/\"vit_clf.joblib\")\n",
        "sc_ae  = joblib.load(Path(MODELS_DIR)/\"ae_scaler.joblib\")\n",
        "clf_ae = joblib.load(Path(MODELS_DIR)/\"ae_clf.joblib\")\n",
        "\n",
        "# Fine-tuned models (OPTIONAL; load if present)\n",
        "def load_ft_model(ckpt_path: Path):\n",
        "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "    arch = ckpt.get(\"arch\",\"\")\n",
        "    ncls = int(ckpt.get(\"num_classes\", 2))\n",
        "    if arch == \"resnet50\":\n",
        "        m = models.resnet50(weights=None)\n",
        "        m.fc = nn.Linear(m.fc.in_features, ncls)\n",
        "    elif arch == \"vit_b16\":\n",
        "        m = models.vit_b_16(weights=None)\n",
        "        m.heads.head = nn.Linear(m.heads.head.in_features, ncls)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown arch in ckpt: {arch}\")\n",
        "    m.load_state_dict(ckpt[\"state_dict\"], strict=True)\n",
        "    return m.eval().to(device)\n",
        "\n",
        "resnet_ft = load_ft_model(Path(MODELS_DIR)/\"resnet50_ft.pt\") if Path(MODELS_DIR, \"resnet50_ft.pt\").exists() else None\n",
        "vit_ft    = load_ft_model(Path(MODELS_DIR)/\"vit_b16_ft.pt\")  if Path(MODELS_DIR, \"vit_b16_ft.pt\").exists()  else None\n",
        "\n",
        "# ---------- Prediction ----------\n",
        "def _norm(x):  return (x - MEAN.to(x.device)) / STD.to(x.device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def _predict_linear_backbones(x):\n",
        "    preds = {}\n",
        "    # resnet linear\n",
        "    out_r = resnet(x)\n",
        "    if out_r.dim()==4: out_r = out_r.mean((2,3))\n",
        "    Xr = out_r.cpu().numpy(); pr = int(clf_r.predict(sc_r.transform(Xr))[0])\n",
        "    cr = float(clf_r.predict_proba(sc_r.transform(Xr)).max(axis=1)[0])\n",
        "    preds[\"resnet50_linear\"] = {\"pred_level\": pr, \"risk\": \"Yes\" if (pr>=1 if not BINARY_MODE else pr==1) else \"No\", \"confidence\": cr}\n",
        "    # vit linear\n",
        "    out_v = vit(x)\n",
        "    if out_v.dim()==4: out_v = out_v.mean((2,3))\n",
        "    Xv = out_v.cpu().numpy(); pv = int(clf_v.predict(sc_v.transform(Xv))[0])\n",
        "    cv = float(clf_v.predict_proba(sc_v.transform(Xv)).max(axis=1)[0])\n",
        "    preds[\"vit_b16_linear\"] = {\"pred_level\": pv, \"risk\": \"Yes\" if (pv>=1 if not BINARY_MODE else pv==1) else \"No\", \"confidence\": cv}\n",
        "    # ae linear\n",
        "    out_a = ae_backbone(x)\n",
        "    if out_a.dim()==4: out_a = out_a.mean((2,3))\n",
        "    Xa = out_a.cpu().numpy(); pa = int(clf_ae.predict(sc_ae.transform(Xa))[0])\n",
        "    ca = float(clf_ae.predict_proba(sc_ae.transform(Xa)).max(axis=1)[0])\n",
        "    preds[\"autoencoder_linear\"] = {\"pred_level\": pa, \"risk\": \"Yes\" if (pa>=1 if not BINARY_MODE else pa==1) else \"No\", \"confidence\": ca}\n",
        "    return preds\n",
        "\n",
        "@torch.no_grad()\n",
        "def _predict_ft(model, x, tta=3, name=\"model_ft\"):\n",
        "    if model is None:\n",
        "        return {}\n",
        "    logits_sum = 0\n",
        "    for t in range(tta):\n",
        "        xi = x\n",
        "        if t % 2 == 1: xi = torch.flip(xi, dims=[3])\n",
        "        if t % 4 == 3: xi = torch.flip(xi, dims=[2])\n",
        "        logits_sum = logits_sum + model(xi)\n",
        "    logits = logits_sum / float(tta)\n",
        "    probs = torch.softmax(logits, dim=1)\n",
        "    conf, pred = probs.max(dim=1)\n",
        "    pred = int(pred.item()); conf = float(conf.item())\n",
        "    risk = \"Yes\" if (pred>=1 if not BINARY_MODE else pred==1) else \"No\"\n",
        "    return {name: {\"pred_level\": pred, \"risk\": risk, \"confidence\": conf}}\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_one_pil(pil_img):\n",
        "    im = ImageOps.exif_transpose(pil_img).convert(\"RGB\").resize((IMAGE_SIZE, IMAGE_SIZE))\n",
        "    arr = np.asarray(im, dtype=np.float32)/255.0\n",
        "    x = torch.from_numpy(np.transpose(arr,(2,0,1))).unsqueeze(0).to(device)\n",
        "    x = _norm(x)\n",
        "\n",
        "    preds = {}\n",
        "    preds.update(_predict_linear_backbones(x))\n",
        "    preds.update(_predict_ft(resnet_ft, x, tta=3, name=\"resnet50_finetune\") if resnet_ft else {})\n",
        "    preds.update(_predict_ft(vit_ft,    x, tta=3, name=\"vit_b16_finetune\") if vit_ft    else {})\n",
        "\n",
        "    # consensus\n",
        "    from collections import Counter\n",
        "    levels = [preds[k][\"pred_level\"] for k in preds]\n",
        "    risks  = [preds[k][\"risk\"] for k in preds]\n",
        "    res = {\n",
        "        \"consensus_level\": int(Counter(levels).most_common(1)[0][0]),\n",
        "        \"consensus_risk\":  Counter(risks).most_common(1)[0][0],\n",
        "        \"all_models_risk\": \"Yes\" if all(r==\"Yes\" for r in risks) else \"No\",\n",
        "        \"per_model\": preds\n",
        "    }\n",
        "    return res\n",
        "\n",
        "# ---------- Pretty formatting ----------\n",
        "def preds_to_markdown(result: dict) -> str:\n",
        "    pm = result.get(\"per_model\", {})\n",
        "    lines = []\n",
        "    lines.append(f\"### Consensus\\n\")\n",
        "    lines.append(f\"- **Level**: `{result['consensus_level']}`\")\n",
        "    lines.append(f\"- **Referable DR risk**: **{result['consensus_risk']}**\")\n",
        "    lines.append(f\"- **All models agree on risk**: **{result['all_models_risk']}**\\n\")\n",
        "    lines.append(\"### Per-model predictions\")\n",
        "    lines.append(\"| Model | Level | Risk | Confidence |\")\n",
        "    lines.append(\"|---|---:|:---:|---:|\")\n",
        "    for name, v in pm.items():\n",
        "        lines.append(f\"| `{name}` | {v['pred_level']} | {v['risk']} | {v['confidence']:.3f} |\")\n",
        "    # Note if some FT models are missing\n",
        "    missing = []\n",
        "    if resnet_ft is None: missing.append(\"resnet50_finetune\")\n",
        "    if vit_ft is None:    missing.append(\"vit_b16_finetune\")\n",
        "    if missing:\n",
        "        lines.append(f\"\\n> _Note: Fine-tuned checkpoints not found for: {', '.join(missing)}. Showing available models only._\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "# ---------- Gradio (auto-terminate after first prediction) ----------\n",
        "try:\n",
        "    import gradio\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"gradio==4.44.0\"])\n",
        "import gradio as gr\n",
        "\n",
        "AUTO_CLOSE_AFTER_PRED = True\n",
        "HARD_EXIT_FALLBACK    = True\n",
        "_shutdown_started = {\"done\": False}\n",
        "demo = None\n",
        "\n",
        "def _graceful_exit():\n",
        "    try:\n",
        "        if demo is not None:\n",
        "            demo.close()\n",
        "            print(\"[gradio] Server closed.\")\n",
        "    except Exception as e:\n",
        "        print(\"[gradio] close() failed:\", e)\n",
        "    sys.stdout.flush()\n",
        "    time.sleep(0.5)\n",
        "    try:\n",
        "        from google.colab import runtime\n",
        "        print(\"[exit] Releasing Colab VM...\")\n",
        "        sys.stdout.flush()\n",
        "        runtime.unassign()\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        import IPython\n",
        "        ip = IPython.get_ipython()\n",
        "        if ip and hasattr(ip, \"kernel\"):\n",
        "            print(\"[exit] Stopping IPython kernel.\")\n",
        "            sys.stdout.flush()\n",
        "            ip.kernel.do_shutdown(restart=False)\n",
        "    except Exception:\n",
        "        pass\n",
        "    if HARD_EXIT_FALLBACK:\n",
        "        try:\n",
        "            print(\"[exit] Hard-exit fallback.\")\n",
        "            sys.stdout.flush()\n",
        "            os._exit(0)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "def ui_fn(img):\n",
        "    if img is None:\n",
        "        return \"No image provided.\"\n",
        "    result = predict_one_pil(img)\n",
        "    md = preds_to_markdown(result)\n",
        "    if AUTO_CLOSE_AFTER_PRED and not _shutdown_started[\"done\"]:\n",
        "        _shutdown_started[\"done\"] = True\n",
        "        threading.Timer(1.5, _graceful_exit).start()\n",
        "    return md\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=ui_fn,\n",
        "    inputs=gr.Image(type=\"pil\", label=\"Upload retina image\"),\n",
        "    outputs=gr.Markdown(label=\"Prediction\"),\n",
        "    title=\"DR Predictor — One-and-done UI\",\n",
        "    allow_flagging=\"never\",\n",
        ")\n",
        "\n",
        "# serve static artifacts from Cell 1 (optional)\n",
        "try:\n",
        "    from starlette.staticfiles import StaticFiles\n",
        "    iface.app.mount(\"/api/files\", StaticFiles(directory=FILES_DIR), name=\"files\")\n",
        "    print(f\"[STATIC] Mounted {FILES_DIR} at /api/files\")\n",
        "except Exception as e:\n",
        "    print(\"[STATIC] Mount skipped:\", e)\n",
        "\n",
        "demo = iface\n",
        "iface.launch(share=True, debug=True, prevent_thread_lock=True)\n",
        "print(\"Ready. After your first prediction, this runtime will shut down automatically.\")\n"
      ],
      "metadata": {
        "id": "6npQshqG1utW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}